# -*- coding: utf-8 -*-
"""Concatenation and Augmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cFmu7CH4gtIAiz_LNq2VwyS64doXl4JG
"""

import torchaudio
import pandas as pd
import numpy as np
import torch

!pip install pydub

from google.colab import drive
drive.mount('/content/drive')

# #################################### CODE FOR CONCATENATING AUDIOS AND TRANSCRIPTS #####################################
# import os
# import pandas as pd
# from pydub import AudioSegment

# # Paths
# dataset_path = '/content/drive/MyDrive/LDCIL_dataset/PunjabiRawSpeechCorpus____/Dataset/Sentence-S/'
# clips_path = os.path.join(dataset_path, 'clips/')
# concatenated_clips_path = os.path.join(dataset_path, 'concatenated_clips/')
# os.makedirs(concatenated_clips_path, exist_ok=True)
# # Ensure tryClips folder exists
# try:
#     os.makedirs(concatenated_clips_path, exist_ok=True)
#     print(f"✅ Folder created or already exists: {concatenated_clips_path}")
# except Exception as e:
#     print(f"❌ Error creating folder: {concatenated_clips_path}")
#     print(e)

# original_train_data = os.path.join(dataset_path, 'train.tsv')
# original_test_data = os.path.join(dataset_path, 'test.tsv')
# combined_transcripts_path = os.path.join(dataset_path, 'concatenated_transcripts.tsv')

# # Load Data
# def load_data(file_path):
#     df = pd.read_csv(file_path, sep='\t', header=None)
#     df.columns = ['audio_file_name', 'transcript']
#     df['audio_file_name'] = df['audio_file_name'].apply(lambda x: os.path.basename(x))
#     return df

# original_train_df = load_data(original_train_data)
# original_test_df = load_data(original_test_data)
# combined_df = pd.concat([original_train_df, original_test_df], ignore_index=True)

# combined_df

# # Concatenate audio and transcripts
# def concatenate_audio_and_transcripts(df, output_tsv_path):
#     concatenated_data = {'audio_file_name': [], 'transcript': []}

#     for i in range(0, len(df), 4):
#         audio_files = df['audio_file_name'][i:i+4].tolist()
#         transcripts = df['transcript'][i:i+4].tolist()

#         # Concatenate audio
#         combined_audio = AudioSegment.silent(duration=0)
#         for audio_file in audio_files:
#             audio_path = os.path.join(clips_path, audio_file)
#             audio = AudioSegment.from_wav(audio_path)
#             combined_audio += audio

#         # Save concatenated audio
#         audio_file_name = f"Audio_File_{i//4 + 1}.wav"
#         combined_audio.export(os.path.join(concatenated_clips_path, audio_file_name), format="wav")

#         # Concatenate transcripts
#         combined_transcript = ' '.join(transcripts)
#         concatenated_data['audio_file_name'].append(audio_file_name)
#         concatenated_data['transcript'].append(combined_transcript)

#     # Save concatenated data to TSV
#     concatenated_df = pd.DataFrame(concatenated_data)
#     concatenated_df.to_csv(output_tsv_path, sep='\t', index=False)

# # Process train and test sets
# concatenate_audio_and_transcripts(combined_df, combined_transcripts_path)

# print("Audio concatenation and transcript merging completed.")

# #################################### CODE FOR GENERATING CONCATENATED TRAIN AND TEST DATASETS #####################################

# import pandas as pd
# from sklearn.model_selection import train_test_split
# import os

# # Path to the input file
# input_path = '/content/drive/MyDrive/LDCIL_dataset/PunjabiRawSpeechCorpus____/Dataset/Sentence-S/concatenated_transcripts.tsv'

# # Load the data
# df = pd.read_csv(input_path, sep='\t')

# # Split the data
# train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# # Make sure the output directory exists
# output_dir = os.path.dirname('/content/drive/MyDrive/LDCIL_dataset/PunjabiRawSpeechCorpus____/Dataset/Sentence-S')
# os.makedirs(output_dir, exist_ok=True)

# # Define output file paths
# train_path = os.path.join(output_dir, 'concatenated_train.tsv')
# test_path = os.path.join(output_dir, 'concatenated_test.tsv')

# # Save the files
# train_df.to_csv(train_path, sep='\t', index=False, header=True)
# test_df.to_csv(test_path, sep='\t', index=False, header=True)

# print(f"Saved:\n- {train_path}\n- {test_path}")

import os
dataset_path = '/content/drive/MyDrive/LDCIL_dataset/PunjabiRawSpeechCorpus____/Dataset/Sentence-S/'
concatenated_clips_path = os.path.join(dataset_path, 'concatenated_clips/')
concatenated_train_data = os.path.join(dataset_path, 'concatenated_train.tsv')
concatenated_test_data = os.path.join(dataset_path, 'concatenated_test.tsv')

import pandas as pd
concatenated_train_df = pd.read_csv(concatenated_train_data, sep='\t', header=0)
concatenated_test_df = pd.read_csv(concatenated_test_data, sep='\t', header=0)

concatenated_train_df.head()

concatenated_train_df['transcript'].iloc[0]

##################################### AUGMENTATIONS ####################

import os
import random
from pydub import AudioSegment, effects
import numpy as np

# Define augmentation functions
def add_noise(audio, noise_level=0.15):
    samples = np.array(audio.get_array_of_samples())
    max_sample = np.max(np.abs(samples))

    # Generate noise once for all samples
    noise = np.random.normal(0, noise_level * max_sample, samples.shape)

    # Add noise and clip to avoid overflow
    noisy_samples = np.clip(samples + noise, -max_sample, max_sample).astype(samples.dtype)

    # Create new audio segment with noisy samples
    return audio._spawn(noisy_samples.tobytes())

def apply_augmentations(audio):
    total_length = len(audio)  # Length in milliseconds
    num_segments = random.randint(3, 6)  # Randomize number of segments between 3 and 6
    segment_points = sorted(random.sample(range(1, total_length), num_segments - 1))
    segment_points = [0] + segment_points + [total_length]

    augmented_audio = AudioSegment.empty()

    for i in range(num_segments):
        start = segment_points[i]
        end = segment_points[i + 1]
        part = audio[start:end]

        # Randomly select one augmentation per part
        effect = random.choice(['speedup', 'noise', 'high_pass', 'gain', None])

        # Apply augmentations only if the segment is long enough
        # Reduced chunk_size to 75ms for speedup
        if effect == 'speedup' and len(part) >= 150:  # 150 ms minimum for speedup
            part = effects.speedup(part, playback_speed=1.1, chunk_size=75)
        elif effect == 'noise':
            part = add_noise(part)
        elif effect == 'high_pass':
            part = part.high_pass_filter(3000)
        elif effect == 'gain':
            part = part.apply_gain(5)

        augmented_audio += part

    return augmented_audio


dataset_path = '/content/drive/MyDrive/LDCIL_dataset/PunjabiRawSpeechCorpus____/Dataset/Sentence-S/'
concatenated_clips_path = os.path.join(dataset_path, 'concatenated_clips/')
augmented_clips_path = os.path.join(dataset_path, 'augmented_clips/')
os.makedirs(augmented_clips_path, exist_ok=True)

# Process the first few audio files in Concatenated Clips
#count = 0
for file_name in os.listdir(concatenated_clips_path):
    if file_name.endswith('.wav'):
        # Load audio from Concatenated Clips
        audio_path = os.path.join(concatenated_clips_path, file_name)
        audio = AudioSegment.from_wav(audio_path)
        # Apply augmentations
        augmented_audio = apply_augmentations(audio)
        # Export to Augmented Clips with the same file name
        augmented_audio.export(os.path.join(augmented_clips_path, file_name), format='wav')
        # count += 1
        # if count >= 2:
        #     break

print("Augmentation complete. Processed files saved in 'Augmented Clips2'.")