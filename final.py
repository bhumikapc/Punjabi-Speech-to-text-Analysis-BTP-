# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z82APrZG8wNL6X_sDHyTARz484b6Ll4P
"""

!pip install datasets

pip install -q transformers datasets accelerate bitsandbytes peft torchaudio

!pip install librosa

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import torch
from datasets import Dataset
from transformers import WhisperProcessor, WhisperForConditionalGeneration, Trainer, TrainingArguments
import torchaudio
from accelerate import Accelerator
import torch.nn as nn

# Set up paths
# Paths
dataset_path = '/content/drive/MyDrive/BTP/'  # Define the base dataset path
clips_path = os.path.join(dataset_path, 'clips/')  # Path to the directory containing audio clips
transcript_file = os.path.join(dataset_path, 'train.tsv')  # Path to the transcript file


# Load transcripts
transcripts = pd.read_csv(transcript_file, sep='\t', header=None, names=['audio_file', 'transcript'])
transcripts['audio_file'] = transcripts['audio_file'].apply(
    lambda x: os.path.join(clips_path, x.strip() + '.wav') if not x.endswith('.wav') else os.path.join(clips_path, x.strip())
)
transcripts = transcripts.head(1)
data = Dataset.from_pandas(transcripts)

torch.cuda.empty_cache()
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Preprocessing: Load audio
def preprocess_data(example):
    audio_path = example['audio_file']
    if not os.path.exists(audio_path):
        return None
    try:
        audio_input, _ = torchaudio.load(audio_path)
        audio_input = audio_input.squeeze().numpy()
    except Exception as e:
        print(f"Error loading {audio_path}: {e}")
        return None
    return {'audio': audio_input, 'text': example['transcript']}

data = data.map(preprocess_data, remove_columns=["audio_file", "transcript"])
data = data.filter(lambda example: example is not None)

# Load model and processor
model_name = "DrishtiSharma/whisper-large-v2-punjabi-700-steps"
processor = WhisperProcessor.from_pretrained(model_name)
model = WhisperForConditionalGeneration.from_pretrained(model_name)

# Freeze all layers
for param in model.parameters():
    param.requires_grad = False

# Replace proj_out with a new trainable linear layer
model.proj_out = torch.nn.Linear(
    model.config.d_model, model.config.vocab_size, bias=False
)
class WhisperModelWrapper(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_features=None, labels=None):
        return self.model(input_features=input_features, labels=labels)

    # Add gradient_checkpointing_enable method to the wrapper
    def gradient_checkpointing_enable(self, **kwargs):
        self.model.gradient_checkpointing_enable(**kwargs)

wrapped_model = WhisperModelWrapper(model)

# Preprocessing function
def preprocess_function(examples):
    inputs = processor.feature_extractor(
        examples["audio"], sampling_rate=16000, return_tensors="pt", padding="max_length", max_length=3000
    )
    input_features = inputs.input_features
    padding_needed = 3000 - input_features.shape[-1]
    if padding_needed > 0:
        input_features = torch.nn.functional.pad(input_features, (0, padding_needed))
    labels = processor.tokenizer(
        examples["text"], return_tensors="pt", padding=True, truncation=True
    ).input_ids
    return {
        "input_features": input_features.squeeze(0).to(torch.float32),
        "labels": labels.squeeze(0),
    }

train_dataset = data.map(preprocess_function)

# Collator
def collate_fn(batch):
    input_features = [torch.tensor(x["input_features"]) for x in batch]
    input_features = torch.stack(input_features)

    labels = [torch.tensor(x["labels"]) for x in batch]
    labels = torch.nn.utils.rnn.pad_sequence(
        labels, batch_first=True, padding_value=processor.tokenizer.pad_token_id
    )
    return {"input_features": input_features, "labels": labels}

# Training args
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=1,
    num_train_epochs=3,
    learning_rate=5e-5,
    gradient_accumulation_steps=64,
    gradient_checkpointing=True,
    fp16=True,
    optim="adamw_torch",
    logging_steps=1,
    save_strategy="epoch",
    report_to="none",
)

# Train
trainer = Trainer(
    model=wrapped_model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=collate_fn,
)

trainer.train()